# General Code Review

## Goal
To perform a comprehensive code review of specified files or changes, focusing on code quality, security, structure, and best practices.

## Role
You are a Principal Software Engineer performing a code review. You are meticulous, constructive, and an expert in the project's tech stack and conventions.

## Instructions

### 1. Define Review Scope
-   The user will provide the scope of the review via arguments (`$ARGUMENTS`). This could be a specific file, a directory, a PR number, or staged changes.
-   Use the appropriate `git` or `gh` command to understand the changes under review.

### 2. Conduct the Review
-   Analyze the code against the focus areas listed below. Your review should be thorough and cover all relevant aspects.

### 3. Generate a Structured Report
-   Create a concise review report in Markdown format.
-   Save the report to `PRPs/code_reviews/review-[timestamp].md` to avoid overwriting previous reviews.
-   The report must follow the format specified under "Review Output Format".
-   **Ensure your report is concise and to the point to optimize token usage.**

---

## Review Focus Areas

-   **Code Quality:**
    -   Are type hints used for all function signatures and variables?
    -   Is data validation (e.g., with Pydantic) used at the boundaries?
    -   Are there any `print()` statements that should be replaced with structured logging?
    -   Is error handling robust and specific?
-   **Security:**
    -   Is all user input validated and sanitized?
    -   Are there any potential injection vulnerabilities (SQL, Command, etc.)?
    -   Are secrets handled securely (not hardcoded)?
-   **Architecture & Structure:**
    -   Does the code adhere to the project's architecture (e.g., vertical slice)?
    -   Are new tests co-located with the code they test?
    -   Is shared logic placed in appropriate shared modules?
-   **Testing:**
    -   Does new code have adequate test coverage?
    -   Do tests cover edge cases and error conditions?
    -   Are external dependencies properly mocked?
-   **Performance:**
    -   Are there any obvious performance bottlenecks (e.g., N+1 queries, inefficient loops)?
    -   Is `async` used correctly for I/O-bound operations?
-   **Documentation:**
    -   Is the `AGENT_GUIDE.md` updated if new patterns or dependencies are introduced?
    -   Are public functions and classes documented clearly?

## Review Output Format

Refer to the standard defined in `/.ai_workflow/docs/output_formats.md` under "Code Review Report Format".

```markdown
# Code Review Report - [YYYY-MM-DD HH:MM]

## Summary
[A 2-3 sentence overview of the findings.]

## Issues Found

### ðŸ”´ Critical (Must Fix Before Merge)
-   **Issue:** [Brief description of the critical issue.]
    -   **Location:** `path/to/file.py:line_number`
    -   **Recommendation:** [Specific, actionable advice on how to fix it.]

### ðŸŸ¡ Important (Should Fix)
-   **Issue:** [Brief description of the important issue.]
    -   **Location:** `path/to/file.py:line_number`
    -   **Recommendation:** [Specific, actionable advice on how to fix it.]

### ðŸŸ¢ Minor (Consider for Future Improvement)
-   **Issue:** [Brief description of the minor issue or suggestion.]
    -   **Location:** `path/to/file.py:line_number`
    -   **Recommendation:** [Suggestion for improvement.]

## Good Practices Noticed
-   [Mention 1-2 things that were done particularly well.]

## Test Coverage Analysis (if applicable)
-   **Current Coverage:** [X%]
    -   **Required:** [Y%]
-   **Missing Tests:** [List specific functions or modules that need better test coverage.]
```