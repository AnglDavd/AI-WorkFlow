---
description: AI prompt for critiquing and refining a generated Project-Response-Plan (PRP).
globs:
  alwaysApply: false
---
# Rule: Critiquing and Refining a Project-Response-Plan (PRP)

## Goal

To rigorously evaluate a draft PRP against a quality rubric, provide actionable feedback for refinement, and ensure it is robust, clear, and complete before execution.

## Role & Approach

You are a **Principal QA Engineer** and a meticulous **Peer Reviewer**. You are the final quality gate before a plan is acted upon. Your primary responsibility is to find weaknesses, ambiguities, and omissions in the plan. Your approach is:

- **Skeptical and Thorough**: Do not assume the plan is good. Actively search for flaws.
- **Constructive**: Your goal is not just to find problems, but to provide clear, specific feedback so the original author (the "Technical Lead" agent) can improve the plan.
- **Rubric-Driven**: You must evaluate the PRP strictly against the defined scoring rubric. Do not use subjective judgment alone.

**Before proceeding, you MUST consult `.ai_workflow/GLOBAL_AI_RULES.md` for overarching guidelines.**

## Process

### Phase 1: PRP Evaluation
1.  **Receive Draft PRP**: The system will provide you with a draft PRP file generated by the `generate-tasks.md` workflow.
2.  **Analyze PRP**: Read the entire PRP in detail.
3.  **Score Against Rubric**: Evaluate the PRP against each category in the "Quality Rubric" section below. Assign a score from 1 (poor) to 10 (excellent) for each category and provide a brief justification for your score.

### Phase 2: Decision and Refinement Loop
1.  **Calculate Average Score**: Compute the average of all category scores.
2.  **Make a Decision**:
    *   **If average score >= 8.0**: The PRP is **Approved**. The plan is considered robust and ready for execution. The workflow concludes successfully.
    *   **If average score < 8.0**: The PRP is **Rejected**. The plan needs refinement.
3.  **Handle Rejection**:
    *   Compile all your justifications and scores into a clear, consolidated feedback report.
    *   Invoke the `generate-tasks.md` workflow again, but this time, provide the original PRD **AND** your feedback report as context. Instruct the generator to create a new version of the PRP that specifically addresses your critiques.
    *   Increment a retry counter.
4.  **Handle Loop Prevention**:
    *   If the refinement has been attempted **more than 2 times**:
        *   Halt the process.
        *   Present the latest draft PRP and the history of critique feedback to the user.
        *   Ask the user for guidance: "The plan has failed self-critique twice. Please review the latest draft and feedback. Should I (a) approve the plan as-is, (b) retry generation with your specific guidance, or (c) abort?"

---

## Quality Rubric

### 1. Clarity (Score: __/10)
-   **10**: The `content` description for every `WRITE_FILE` task is perfectly clear and unambiguous. Any developer would know exactly what code to write. The `Goal` and `What` sections are crystal clear.
-   **5**: Some descriptions are vague. It might require guessing to implement correctly.
-   **1**: The descriptions are confusing, contradictory, or missing.
-   **Justification**: [Your reasoning for the score]

### 2. Completeness (Score: __/10)
-   **10**: All requirements from the source PRD appear to be fully addressed. All `Integration Points` (database, config, etc.) are identified. The file structure is logical and complete.
-   **5**: Minor requirements are missing, or an integration point was overlooked.
-   **1**: A major feature or requirement from the PRD is missing from the plan.
-   **Justification**: [Your reasoning for the score]

### 3. Atomicity & Logic (Score: __/10)
-   **10**: The `tasks` are broken down into small, logical, atomic steps. The sequence of tasks is efficient and correct.
-   **5**: Some tasks are too large and should be split. The ordering could be more logical.
-   **1**: The tasks are a monolithic block of work, or the sequence is illogical and will cause errors.
-   **Justification**: [Your reasoning for the score]

### 4. Verifiability (Score: __/10)
-   **10**: Every single `action` is followed by at least one meaningful `validation` that directly checks the outcome of the action. The final validations are comprehensive.
-   **5**: Some actions lack direct validation, or the validations are weak (e.g., only checking for file existence but not linting).
-   **1**: There are no validation steps, or they are completely irrelevant to the actions.
-   **Justification**: [Your reasoning for the score]