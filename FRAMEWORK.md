# AI Development Framework - Memory & Best Practices

**Framework Version:** v3.1.1 Enhanced with Context7 Integration  
**Last Updated:** 2025-01-20 (Repository Structure Streamlined)  
**Status:** Production Ready with Ultra-Flat Structure and Iterative Quality Loop

---

## üéØ Framework Overview

This is an ultra-efficient AI development framework optimized for speed, quality, and real-world results. The framework has been enhanced with:

- **Smart contextual questioning** (60% time reduction in PRD creation)
- **Auto-completion with Context7** research integration
- **Realistic automatic estimates** based on current market rates
- **Iterative quality loop** until 8/10+ achieved across all dimensions
- **MCP integration** for Playwright visual analysis and Context7 research
- **Ultra-flat repository structure** for simplified maintenance and contribution
- **Streamlined architecture** with removed complex GitHub workflows

---

## üöÄ Quick Start Commands

### Core Workflow Commands
```bash
# 1. Create PRD (Enhanced with smart questioning)
./ai-dev create

# 2. Generate Tasks (Enhanced with Context7 validation)
./ai-dev generate 01_prd_{session-id}_{project-name}.md

# 3. Execute Implementation (Enhanced with iterative quality)
./ai-dev execute 02_tasks_{session-id}_{project-name}.md

# 4. Run Iterative Quality Loop (NEW - replaces single healing)
./ai-dev iterate quality {session-id}

# 5. Force healing analysis (legacy support)
./ai-dev heal {session-id}
```

### Management Commands
```bash
# List all sessions
./ai-dev list

# Show session details
./ai-dev status {session-id}

# Resume working on a session
./ai-dev resume {session-id}

# Archive completed session
./ai-dev archive {session-id}
```

---

## üìã Enhanced Workflow Features

### 1. Smart PRD Creation
- **Auto-detects project type** in 1 question (vs 15+ follow-ups)
- **Context-specific questions** (6-8 vs 20+ generic)
- **Auto-completion** with Context7 research
- **Realistic estimates** based on current market rates

**Key Efficiency Gains:**
- Interview time: 15 minutes (was 45+ minutes)
- Higher quality responses due to relevant context
- Auto-populated tech stack suggestions
- Market-validated cost and timeline estimates

### 2. Enhanced Task Generation
- **MCP Context7 validation** of tech stacks
- **Current best practices** research integration
- **Automatic complexity determination**
- **Cross-validation** with PRD requirements

### 3. Iterative Quality Execution
- **Maximum 5 iterations** until 8/10+ achieved
- **Automatic improvement** generation and application
- **Context7-enhanced** standards research
- **Quality certification** before completion

---

## üîß MCP Integration Requirements

### Required MCP Tools
```bash
# Install MCP Playwright for UI analysis
claude mcp add playwright npx '@playwright/mcp@latest'

# Verify installation
claude mcp list | grep playwright
```

### Simplified Repository Structure
- **Ultra-flat design** - All core files in root directory
- **No complex workflows** - Removed GitHub Actions for easier maintenance
- **Direct access** - All guides and tools immediately accessible
- **Reduced dependencies** - Eliminated pre-commit conflicts and CI/CD complexity

### Context7 Integration
- Used for real-time research of current best practices
- Validates tech stack choices against 2025 trends
- Provides market rate data for realistic estimates
- Enhances quality standards with latest industry benchmarks

---

## üìä Quality Standards

### Mandatory Quality Thresholds (ALL must be >= 8/10)
- **Visual Consistency:** 20% weight - Design system adherence
- **CRO Optimization:** 25% weight - Conversion rate optimization  
- **Accessibility:** 20% weight - WCAG 2.1 compliance
- **Architecture Quality:** 15% weight - Code structure quality
- **Performance:** 10% weight - Load times and optimization
- **Responsive Design:** 10% weight - Multi-device support

### Quality Enforcement
- **Automatic blocking** if any dimension < 8/10
- **Iterative improvement** until threshold met
- **Maximum 5 iterations** before manual review required
- **Quality certification** seal for approved projects

---

## üé® Project Complexity Detection

### Automatic Classification
```bash
Simple      | 10-15 tasks  | $2K-$10K   | 3-8 weeks  | 1-2 devs
Medium      | 15-25 tasks  | $10K-$25K  | 8-16 weeks | 2-3 devs  
Complex     | 25-40 tasks  | $25K-$60K  | 16-24 weeks| 3-5 devs
Enterprise  | 40+ tasks    | $60K+      | 24+ weeks  | 5+ devs
```

### Tech Stack Recommendations
- **React/Next.js:** +15% premium (high demand)
- **WordPress:** -10% efficiency (rapid development)
- **Laravel/Django:** Standard rates
- **Enterprise features:** +20% (compliance overhead)

---

## üìÅ File Structure & Session Management

### Session ID Format
- **Pattern:** `{timestamp-3digits}{random-3digits}` (e.g., 592846)
- **Collision detection** and auto-regeneration
- **Perfect traceability** across all workflow files

### Generated Files
```
01_prd_{session-id}_{project-name}.md      # PRD with smart questioning
02_tasks_{session-id}_{project-name}.md    # Tasks with Context7 validation
03_report_{session-id}_{project-name}.md   # Execution report
03_quality_{session-id}_{project-name}.md  # Quality certification
```

---

## ‚ö° Performance Optimizations

### PRD Creation Efficiency
- **60% time reduction** through smart questioning
- **Auto-completion** reduces technical decisions from 30+ min to 5 min
- **Market research** provides realistic estimates instantly
- **Context-aware** suggestions based on project type

### Task Generation Enhancement
- **Context7 validation** of tech stack patterns
- **Current best practices** integration
- **Automatic feasibility** validation
- **Cross-PRD consistency** checking

### Execution Improvements
- **Iterative quality loop** replaces single-pass healing
- **Automatic improvement** generation and application
- **Real-time research** integration during iterations
- **Quality certification** with professional standards

---

## üîÑ Iterative Quality Loop Details

### Loop Execution
1. **Comprehensive audit** with MCP Playwright + Context7
2. **Score calculation** across all 6 dimensions
3. **Threshold validation** (all >= 8/10)
4. **Research improvement** strategies if needed
5. **Generate targeted** improvements
6. **Apply automatic** fixes
7. **Repeat until** quality achieved (max 5 iterations)

### Success Criteria
- ‚úÖ ALL dimensions >= 8/10
- ‚úÖ Professional quality standards exceeded
- ‚úÖ Production deployment approved
- ‚úÖ Quality certification seal applied

---

## üõ†Ô∏è Common Usage Patterns

### New Project Workflow
```bash
# 1. Start with smart PRD creation
./ai-dev create
# ‚Üí Answer 6-8 contextual questions (15 min)
# ‚Üí Get auto-completed tech stack suggestions
# ‚Üí Receive realistic market-based estimates

# 2. Generate enhanced task plan
./ai-dev generate 01_prd_{session-id}_{project-name}.md
# ‚Üí Context7-validated tech stack patterns
# ‚Üí Current best practices integration
# ‚Üí Automatic complexity determination

# 3. Execute with iterative quality
./ai-dev execute 02_tasks_{session-id}_{project-name}.md
# ‚Üí Automatic iterative quality loop
# ‚Üí Maximum 5 iterations until 8/10+
# ‚Üí Quality certification before completion
```

### Resume Existing Session
```bash
# Check session status
./ai-dev status {session-id}

# Resume where left off
./ai-dev resume {session-id}

# Force quality check if needed
./ai-dev iterate quality {session-id}
```

### Quality Validation
```bash
# Run quality analysis independently
./ai-dev iterate quality {session-id}

# Check if project meets standards
./ai-dev status {session-id} --quality

# Generate quality report
./ai-dev report {session-id} --quality
```

---

## üìä Complete Framework Structure (Hierarchical Overview)

### 1. **FRAMEWORK CORE**
   1.1. **Main CLI** (`ai-dev`)
       - 1.1.1. Core Commands (create, generate, execute, iterate)
       - 1.1.2. Management Commands (list, status, resume, archive)
       - 1.1.3. Session ID Generation & Validation
   
   1.2. **Session Management System**
       - 1.2.1. Collision-resistant Session IDs (`{timestamp-3}{random-3}`)
       - 1.2.2. Perfect File Traceability
       - 1.2.3. Cross-file Consistency Validation

### 2. **WORKFLOW PHASES** (Sequential, Session-linked)
   2.1. **Phase 1: Smart PRD Creation** (`create_prd_guide.md`)
       - 2.1.1. **Smart Project Classification** (1 question ‚Üí project type)
       - 2.1.2. **Context-Specific Questioning** (6-8 vs 20+ questions)
           - 2.1.2.1. E-commerce: 5 focused questions
           - 2.1.2.2. SaaS: 6 focused questions  
           - 2.1.2.3. Blog/Corporate: 4 focused questions
           - 2.1.2.4. Mobile: 5 focused questions
           - 2.1.2.5. API: 4 focused questions
       - 2.1.3. **Auto-Completion with Context7**
           - 2.1.3.1. Tech Stack Suggestions (based on project type + scale)
           - 2.1.3.2. Current 2025 Trends Research
           - 2.1.3.3. User Accept/Modify/Reject Flow
       - 2.1.4. **Realistic Automatic Estimates**
           - 2.1.4.1. Market Rate Research via Context7
           - 2.1.4.2. Technology-Specific Adjustments
           - 2.1.4.3. Feature-Based Cost Breakdowns
           - 2.1.4.4. Team Size & Timeline Recommendations
       
   2.2. **Phase 2: Enhanced Task Generation** (`generate_tasks_guide.md`)
       - 2.2.1. **Automatic Complexity Determination**
           - 2.2.1.1. Simple: 10-15 tasks, $2K-$10K, 1-2 devs
           - 2.2.1.2. Medium: 15-25 tasks, $10K-$25K, 2-3 devs
           - 2.2.1.3. Complex: 25-40 tasks, $25K-$60K, 3-5 devs
           - 2.2.1.4. Enterprise: 40+ tasks, $60K+, 5+ devs
       - 2.2.2. **Context7 Validation & Research**
           - 2.2.2.1. Tech Stack Pattern Validation
           - 2.2.2.2. Current Best Practices Integration
           - 2.2.2.3. Industry-Specific Standards Check
       - 2.2.3. **Task Structure Enhancement**
           - 2.2.3.1. 8-40 hour granularity per task
           - 2.2.3.2. Cross-PRD consistency validation
           - 2.2.3.3. Technical feasibility validation

   2.3. **Phase 3: Iterative Quality Execution** (`execute_tasks_guide.md`)
       - 2.3.1. **Standard Implementation Phases** (1-5)
           - 2.3.1.1. Phase 1: Foundation & Setup
           - 2.3.1.2. Phase 2: Core Feature Development  
           - 2.3.1.3. Phase 3: Quality Assurance & Testing
           - 2.3.1.4. Phase 4: Deployment & Production
           - 2.3.1.5. Phase 5: Monitoring & Maintenance
       - 2.3.2. **Mandatory Iterative Quality Loop** (replaces single healing)
           - 2.3.2.1. Maximum 5 iterations until 8/10+ achieved
           - 2.3.2.2. Automatic improvement generation
           - 2.3.2.3. Context7-enhanced standards research
           - 2.3.2.4. Quality certification before completion

### 3. **QUALITY SYSTEM** (Iterative, MCP-powered)
   3.1. **UI/UX Iterative Quality System** (`ui_healing_guide.md`)
       - 3.1.1. **MCP Tool Requirements**
           - 3.1.1.1. MCP Playwright (automated browser testing)
           - 3.1.1.2. MCP Context7 (real-time research)
           - 3.1.1.3. Additional MCPs (web-scraper, lighthouse, accessibility)
       - 3.1.2. **6 Quality Dimensions** (ALL must be >= 8/10)
           - 3.1.2.1. **Visual Consistency** (20% weight)
               - Design system adherence
               - Component consistency analysis
               - Brand alignment assessment
           - 3.1.2.2. **CRO Optimization** (25% weight - highest)
               - CTA placement & effectiveness
               - Trust signal implementation
               - Conversion funnel analysis
           - 3.1.2.3. **Accessibility** (20% weight)
               - WCAG 2.1 compliance
               - Color contrast validation
               - Keyboard navigation testing
           - 3.1.2.4. **Architecture Quality** (15% weight)
               - Frontend architecture assessment
               - API design evaluation
               - Code organization analysis
           - 3.1.2.5. **Performance** (10% weight)
               - Lighthouse integration
               - Core Web Vitals measurement
               - Bundle optimization analysis
           - 3.1.2.6. **Responsive Design** (10% weight)
               - Multi-breakpoint testing
               - Touch target validation
               - Cross-device consistency
       - 3.1.3. **Iterative Improvement Loop**
           - 3.1.3.1. Enhanced analysis with Context7 research
           - 3.1.3.2. Score calculation with iteration weighting
           - 3.1.3.3. Threshold validation (8/10 requirement)
           - 3.1.3.4. Automatic improvement application
           - 3.1.3.5. Success/Manual review determination

### 4. **SUPPORTING SYSTEMS**
   4.1. **Standards & Patterns** (JSON Configuration)
       - 4.1.1. **UI Healing Standards** (`ui_healing_standards.json`)
           - Technical standards and scoring matrix
           - Quality thresholds and criteria
       - 4.1.2. **CRO Optimization Patterns** (`cro_optimization_patterns.json`)
           - 88 proven conversion optimization patterns
           - Industry-specific CRO implementations
   
   4.2. **Integration Layer**
       - 4.2.1. **MCP Context7 Integration**
           - Real-time best practices research
           - Market rate validation
           - Technology trend analysis
       - 4.2.2. **MCP Playwright Integration**
           - Automated visual analysis
           - Multi-breakpoint testing
           - Performance measurement

### 5. **OUTPUT ARTIFACTS** (Session-linked files)
   5.1. **Generated Documents**
       - 5.1.1. `01_prd_{session-id}_{project-name}.md` (Enhanced PRD)
       - 5.1.2. `02_tasks_{session-id}_{project-name}.md` (Validated tasks)
       - 5.1.3. `03_report_{session-id}_{project-name}.md` (Execution report)
       - 5.1.4. `03_quality_{session-id}_{project-name}.md` (Quality certification)
   
   5.2. **Quality Certification**
       - 5.2.1. Professional standards validation
       - 5.2.2. Production deployment approval
       - 5.2.3. Iteration history documentation

### 6. **EFFICIENCY METRICS** (Measurable improvements)
   6.1. **Time Reductions**
       - 6.1.1. PRD Creation: 15 min (was 45+ min) ‚Üí 60% reduction
       - 6.1.2. Tech Decisions: 5 min (was 30+ min) ‚Üí 83% reduction
       - 6.1.3. Quality Assurance: Automated (was manual) ‚Üí 100% automation
   
   6.2. **Quality Improvements**
       - 6.2.1. 100% compliance with 8/10+ standard
       - 6.2.2. Professional certification for all projects
       - 6.2.3. Systematic quality validation before deployment

---

## üéØ Best Practices for Claude

### Before Starting Any Project
1. **Always check** for existing sessions first
2. **Use smart questioning** - let the framework detect project type
3. **Trust auto-completion** - it's based on current market research
4. **Don't skip quality loop** - it ensures professional standards

### During PRD Creation
1. **Answer contextual questions** thoroughly but concisely
2. **Review tech stack suggestions** before accepting
3. **Validate estimates** against project scope
4. **Use Context7 research** for current trends

### During Task Generation
1. **Let Context7 validate** tech stack patterns
2. **Check complexity classification** is accurate
3. **Ensure cross-validation** with PRD passes
4. **Verify task granularity** (8-40 hours per task)

### During Execution
1. **Trust the iterative process** - let it run its course
2. **Don't bypass quality checks** - they prevent technical debt
3. **Document any manual interventions** needed
4. **Ensure quality certification** before declaring complete

### Before Any Code Changes
1. **Always explain first** - Provide short, concise explanation of what the change does
2. **State the purpose** - Why this change is needed or beneficial
3. **Describe the impact** - What will be different after the change
4. **Then implement** - Only proceed with code after explanation is given

---

## üö® Error Handling & Troubleshooting

### Common Issues
- **Session ID collisions:** Framework auto-resolves
- **Context7 research failures:** Graceful fallback to cached patterns
- **Quality loop timeout:** Manual improvement plan generated
- **MCP Playwright issues:** Verify installation and permissions

### Emergency Commands
```bash
# Reset session if corrupted
./ai-dev reset {session-id}

# Force manual quality review
./ai-dev heal {session-id} --manual

# Export session for debugging
./ai-dev export {session-id}
```

---

## üîÑ Git Workflow Guidelines

### Mandatory Git Practices (ALWAYS follow)
1. **Commit after each major milestone** - PRD creation, task generation, phase completion
2. **Proactive commit reminders** - Assistant MUST remind user to commit after completing significant work
3. **Conventional commit format** - Use: `feat:`, `fix:`, `docs:`, `refactor:`, `test:`
4. **Session traceability** - Include session-id in commit messages for perfect tracking
5. **Backup frequency** - Push to remote after each completed workflow phase

### Commit Timing Requirements
- **After PRD creation** - `feat: add PRD for {session-id} {project-name}`
- **After task generation** - `feat: add task breakdown for {session-id} {project-name}`
- **After each implementation phase** - `feat: complete phase {N} for {session-id}`
- **After quality certification** - `feat: achieve quality certification for {session-id}`
- **Before session end** - `docs: finalize session {session-id} documentation`

### Assistant Behavior Rules
1. **ALWAYS remind** user to commit after completing any file generation
2. **Suggest specific** commit messages following conventional format
3. **Include session-id** in all suggested commit messages
4. **Recommend push** to remote after major milestones
5. **Never assume** Git is configured - always check and guide setup if needed

### Git Setup Validation
```bash
# Check if Git is initialized
git status

# If not initialized, guide user through:
git init
git add .
git commit -m "feat: initialize project with framework files"

# For remote backup (optional but recommended):
git remote add origin <repository-url>
git push -u origin main
```

### Example Workflow Integration
```bash
# 1. Create PRD
./ai-dev create
# ‚Üí Assistant: "Great! Now let's commit this PRD: git add . && git commit -m 'feat: add PRD for {session-id} {project-name}'"

# 2. Generate tasks  
./ai-dev generate 01_prd_{session-id}_{project-name}.md
# ‚Üí Assistant: "Task generation complete! Let's save this progress: git add . && git commit -m 'feat: add task breakdown for {session-id} {project-name}'"

# 3. Execute phases
./ai-dev execute 02_tasks_{session-id}_{project-name}.md
# ‚Üí Assistant: "Phase 1 complete! Time to commit: git add . && git commit -m 'feat: complete phase 1 foundation for {session-id}'"
```

---

## üìä Execution Tracking System

### Mandatory Task Completion Reporting
Every execution session MUST maintain real-time progress tracking through the execution report file.

### **Report File Creation (MANDATORY)**
```
03_report_{session-id}_{project-name}.md
```

**When to create:** Immediately upon starting task execution (before first task)  
**When to update:** After EVERY single task completion  
**Format:** Structured markdown with checkboxes and progress tracking

### **Required Report Structure (ISSUE-007 Resolution)**
```markdown
# Project Execution Report
**Project:** {project-name}
**Session ID:** {session-id}
**Started:** {inicio}
**Last Updated:** {timestamp actual}
**Framework:** AI Development Framework v3.1.1

## üìã Development Phase Progress

### Phase 1: Foundation & Setup
- [ ] Task 1.1: Description - Status: Pending
- [x] Task 1.2: Description - Status: COMPLETED ‚úÖ

### Phase 2: Core Development  
- [ ] Task 2.1: Description - Status: In Progress üîÑ
- [ ] Task 2.2: Description - Status: Pending

### Phase 3: Testing & Integration
- [ ] Task 3.1: Description - Status: Pending
- [ ] Task 3.2: Description - Status: Pending

### Phase 4: Deployment Preparation
- [ ] Task 4.1: Description - Status: Pending
- [ ] Task 4.2: Description - Status: Pending

### Phase 5: Production Deployment
- [ ] Task 5.1: Description - Status: Pending
- [ ] Task 5.2: Description - Status: Pending

## üîÑ MANDATORY Quality Assurance & Healing Phase

### Phase 6: Iterative Quality Loop (REQUIRED)
- [ ] **QA-1:** Initial quality analysis with MCP Playwright
- [ ] **QA-2:** Context7 research for current best practices  
- [ ] **QA-3:** 6-dimension scoring (all must be >= 8/10)
  - [ ] QA-3.1: Visual Consistency (20% weight) - Score: __/10
  - [ ] QA-3.2: CRO Optimization (25% weight) - Score: __/10  
  - [ ] QA-3.3: Accessibility (20% weight) - Score: __/10
  - [ ] QA-3.4: Architecture Quality (15% weight) - Score: __/10
  - [ ] QA-3.5: Performance (10% weight) - Score: __/10
  - [ ] QA-3.6: Responsive Design (10% weight) - Score: __/10
- [ ] **QA-4:** Threshold validation (ALL >= 8/10?)
- [ ] **QA-5:** Generate improvement recommendations (if needed)
- [ ] **QA-6:** Apply automatic improvements (Iteration 1)
- [ ] **QA-7:** Re-analyze and re-score (Iteration 1)
- [ ] **QA-8:** Apply improvements (Iteration 2, if needed)
- [ ] **QA-9:** Final quality certification (Max 5 iterations)
- [ ] **QA-10:** Generate 03_quality_{session-id}_{project-name}.md

### üèÜ Quality Certification Status
**Current Iteration:** 0 (Not Started)  
**Highest Scores Achieved:**
- Visual Consistency: __/10
- CRO Optimization: __/10  
- Accessibility: __/10
- Architecture Quality: __/10
- Performance: __/10
- Responsive Design: __/10

**Certification Status:** ‚ùå PENDING - Quality loop not completed
**Can Deploy to Production:** ‚ùå NO - Must achieve 8/10+ in ALL dimensions

## ‚úÖ Completed Tasks Log
**[2025-01-20 14:30]** - Task 1.2: Setup development environment - COMPLETED
**[2025-01-20 15:45]** - Task 2.1: Create database models - COMPLETED

## ‚úÖ Quality Loop Log
**[Timestamp]** - QA-X: Description of quality step - COMPLETED
**[Timestamp]** - Iteration 1 Analysis: Scores [8,7,9,8,8,9] - 1 dimension below threshold
**[Timestamp]** - Iteration 1 Improvements: Applied CRO optimizations
**[Timestamp]** - Iteration 2 Analysis: Scores [8,8,9,8,8,9] - ALL PASS ‚úÖ

## üìä Overall Progress Summary
- **Development Tasks:** X/Y completed (Z%)
- **Quality Assurance:** X/10 steps completed (Z%)
- **Overall Project:** X/Y total items completed (Z%)
- **Quality Certification:** ‚úÖ ACHIEVED / ‚ùå PENDING / üîÑ IN PROGRESS
- **Production Ready:** ‚úÖ YES / ‚ùå NO
```

### **Assistant Behavior Requirements**

#### **1. Report Initialization**
- **IMMEDIATELY** after receiving execute command, create the report file
- Load all tasks from 02_tasks file into report structure
- Set all tasks to "Pending" status initially
- Create timestamp for session start

#### **2. Real-Time Updates (CRITICAL)**
- **After completing ANY development task:** Update report file immediately
- **After completing ANY quality task:** Update both development and quality sections
- Change task status from [ ] to [x] 
- Add entry to appropriate Log (Completed Tasks or Quality Loop)
- Update Progress Summary percentages for BOTH development and quality
- **NEVER forget** to update the report
- **NEVER skip** quality phase tracking - it's mandatory

#### **3. Progress Communication**
```bash
# After each development task completion:
"‚úÖ Task X.Y completed! Updating execution report..."
"üìä Development Progress: X/Y tasks completed (Z%)"
"üíæ Report updated: 03_report_{session-id}_{project-name}.md"
"üîÑ Next task: Task X.Z - [description]"

# After each quality task completion:
"‚úÖ QA-X completed! Updating quality tracking..."
"üéØ Quality Progress: X/10 QA steps completed (Z%)"
"üìä Current Scores: [Visual:X, CRO:Y, Access:Z, Arch:A, Perf:B, Resp:C]"
"üîÑ Next quality step: QA-X - [description]"

# When all development done but quality pending:
"üéâ Development phase complete! Starting mandatory quality loop..."
"‚ö†Ô∏è  Project NOT production-ready until quality certification achieved"
"üîÑ Beginning Phase 6: Quality Assurance & Healing"
```

#### **4. Commit Integration**
- Include report updates in commit messages for BOTH development and quality
- Development example: `feat: complete task 1.2 and update execution tracking for {session-id}`
- Quality example: `feat: complete QA-3 scoring and update quality tracking for {session-id}`
- Final example: `feat: achieve quality certification and finalize project {session-id}`

### **Quality Gates for Execution Tracking (ISSUE-007 Resolution)**
- **No development task completion** without report update
- **No quality task completion** without quality section update
- **No phase advancement** without updated progress summary for ALL phases
- **No development completion** without starting Phase 6 (Quality Loop)
- **No session completion** without 100% development AND quality completion
- **No production deployment** without quality certification achieved (8/10+ all dimensions)
- **Mandatory final commit** including complete execution report with quality certification

### **Error Prevention**
- Validate report file exists before starting any task
- Verify task completion is reflected in report before moving to next
- Cross-check task count in report matches task file
- Ensure progress percentages are mathematically correct

---

## ‚úÖ Task Completion Validation System

### CRITICAL: Definition of Done (DoD) Requirements
**Purpose:** Prevent false completion reporting (ISSUE-005)  
**Enforcement:** MANDATORY before marking any task as complete

### **Universal DoD Criteria (ALL tasks must meet these)**
1. **Functional Requirement Met** - The specific deliverable exists and works
2. **Acceptance Criteria Satisfied** - All task requirements fulfilled completely  
3. **Dependencies Resolved** - No blockers remain for subsequent tasks
4. **Validation Evidence** - Proof of completion available (files, screenshots, tests)
5. **Integration Verified** - Works correctly with existing project components

### **Task-Type Specific DoD Criteria**

#### **Development Tasks**
- [ ] **Code Written** - All required code implemented
- [ ] **Code Tested** - Manual verification or automated tests pass
- [ ] **Code Documented** - Comments and documentation added where needed
- [ ] **Dependencies Installed** - All required packages/libraries working
- [ ] **Integration Working** - New code integrates with existing codebase
- [ ] **No Breaking Changes** - Existing functionality still works

#### **Configuration Tasks**
- [ ] **Configuration Applied** - Settings actually changed in target system
- [ ] **Configuration Tested** - New settings work as expected
- [ ] **Backup Created** - Previous configuration saved if needed
- [ ] **Documentation Updated** - Configuration changes documented
- [ ] **Permissions Set** - Correct access rights applied

#### **File/Content Creation Tasks**
- [ ] **File Created** - Target file exists at specified location
- [ ] **Content Complete** - All required content included
- [ ] **Format Correct** - File format and structure as specified
- [ ] **Permissions Set** - File accessible to required users/systems
- [ ] **Content Validated** - Content quality checked and approved

#### **Setup/Installation Tasks**
- [ ] **Software Installed** - Target software successfully installed
- [ ] **Installation Verified** - Software runs without errors
- [ ] **Dependencies Met** - All prerequisites satisfied
- [ ] **Configuration Applied** - Initial setup completed
- [ ] **Functionality Tested** - Basic operations work correctly

### **Mandatory Validation Protocol**

#### **Before Marking ANY Task Complete - MUST Execute:**

```markdown
## üîç Task Completion Validation Checklist

**Task ID:** [X.Y]
**Task Description:** [Brief description]
**Attempted Completion Date:** [Timestamp]

### Step 1: Universal DoD Verification
- [ ] Functional requirement met
- [ ] Acceptance criteria satisfied  
- [ ] Dependencies resolved
- [ ] Validation evidence exists
- [ ] Integration verified

### Step 2: Task-Type Specific DoD
[Use appropriate checklist from above]

### Step 3: Evidence Documentation
**Proof of Completion:**
- [ ] Screenshot/file evidence attached
- [ ] Testing results documented
- [ ] Error logs reviewed (if applicable)
- [ ] Dependencies verified working

### Step 4: Final Validation Decision
**DECISION:** 
- [ ] ‚úÖ COMPLETE - All DoD criteria met, task genuinely finished
- [ ] üîÑ BLOCKED - Cannot complete due to impediment (use impediment protocol)
- [ ] ‚ùå FAILED - Attempted but unable to satisfy requirements

**IF NOT COMPLETE:** Task remains marked as [ ] and impediment is reported
**IF COMPLETE:** Task can be marked as [x] and logged with evidence
```

### **Impediment Resolution Protocol - Flexible Approach (ISSUE-008 Resolution)**

#### **Core Principle: Always Try Before Escalating**

#### **When Tasks Cannot Be Completed:**

```markdown
## üö´ Task Impediment Report

**Task ID:** [X.Y]
**Task Description:** [Brief description]
**Impediment Discovered:** [Timestamp]

### Impediment Classification
**Type:** [Technical/Dependency/Resource/Knowledge]
**Description:** [Detailed explanation of what's blocking completion]
**Complexity Assessment:** [Simple/Moderate/Complex/Expert-Level]
**Impact:** [How this affects the project timeline]

### Resolution Attempts (Adaptive Approach)
**Minimum Required:** Try at least 2 different approaches before escalating

**Attempt 1:** [Approach description] - Result: [Success/Partial/Failed - explain]
**Attempt 2:** [Different approach] - Result: [Success/Partial/Failed - explain]
**Attempt 3:** [If needed] - Result: [Success/Partial/Failed - explain]

### Adaptive Resolution Strategy
**For Simple Issues (5-15 min effort):**
- [ ] Quick research: Check docs/common solutions
- [ ] Try obvious alternative approach
- [ ] If no progress ‚Üí escalate with clear description

**For Moderate Issues (15-30 min effort):**
- [ ] Research multiple approaches online
- [ ] Try 2-3 different implementation methods
- [ ] Break down into smaller sub-problems
- [ ] If significant progress ‚Üí continue; if stuck ‚Üí escalate

**For Complex Issues (30+ min potential):**
- [ ] Assess if this is core to project success
- [ ] If YES: Invest more time with systematic approaches
- [ ] If NO: Look for simpler alternatives or workarounds
- [ ] Document findings and escalate with research done

### Smart Escalation
**When to Escalate (ANY of these conditions):**
- [ ] No progress after reasonable attempts (15-30 min typically)
- [ ] Issue requires expertise clearly outside AI knowledge
- [ ] Problem is environmental/access-related
- [ ] Multiple similar approaches all fail for same reason
- [ ] Solution would take longer than implementing different approach

**Escalation Information:**
**Specific Help Needed:** [Be very clear about what's needed]
**What Was Tried:** [Brief summary of attempts]
**Why It's Blocked:** [Root cause if identified]
**Suggested Next Steps:** [What human should try]

### Resolution Outcome
**Final Status:** 
- [ ] ‚úÖ RESOLVED - [How it was solved]
- [ ] üîÑ WORKAROUND - [Alternative approach used]
- [ ] üÜò ESCALATED - [Awaiting human help]
- [ ] üîÄ DEFERRED - [Will revisit later]

**If Resolved:**
**Solution:** [What actually worked]
**Time Invested:** [Rough time spent]
**Key Insight:** [What made the difference]
```

#### **Flexible Assistant Behavior Guidelines**

##### **Smart Decision Making:**
1. **Assess impediment complexity** realistically
2. **Match effort to importance** - core features get more time
3. **Know when to pivot** - try alternatives vs banging head on wall
4. **Escalate intelligently** - with useful context, not just "it's broken"

##### **Effort Guidelines (Not Strict Rules):**
- **Quick fixes:** 5-10 minutes of attempts
- **Standard issues:** 15-30 minutes before considering escalation
- **Critical path items:** Up to 45 minutes if making progress
- **Nice-to-have features:** 15 minutes max, then find alternatives

##### **Resolution Mindset:**
- **Goal:** Make project progress, not perfect solutions
- **Acceptable:** Working alternatives that meet core requirements
- **Preferred:** Document learnings for future similar issues
- **Smart:** Balance time investment with project priorities
```

### **Assistant Behavior Enforcement**

#### **NEVER Allow These Actions:**
1. ‚ùå Marking task complete without DoD validation
2. ‚ùå Moving to next task while previous task blocked
3. ‚ùå Reporting "attempted" as "completed"
4. ‚ùå Skipping impediment reporting when blocked
5. ‚ùå Making assumptions about task success without evidence

#### **ALWAYS Require These Actions:**
1. ‚úÖ Complete DoD checklist before marking any task done
2. ‚úÖ Document evidence of successful completion
3. ‚úÖ Report impediments immediately when discovered
4. ‚úÖ Verify dependencies are satisfied before advancing
5. ‚úÖ Update execution report with accurate status only

### **Quality Gates**
- **No task marked complete** without passing DoD validation
- **No phase advancement** with any blocked tasks unresolved
- **No session completion** with any false completions
- **Mandatory impediment review** before any workarounds

---

## üé≠ Mandatory Playwright Integration Protocol (ISSUE-010 Resolution)

### CRITICAL: AI Must Actually USE Playwright for Quality Analysis

### **Problem: AI Claims to Use Playwright But Doesn't**
‚ùå AI provides quality scores without actually using Playwright  
‚ùå No screenshots or visual evidence provided  
‚ùå Subjective assessment instead of systematic analysis  
‚ùå Quality certification without actual browser testing  

### **MANDATORY Playwright Usage Protocol**

#### **Phase 6: Quality Analysis - Playwright Integration Required**

##### **QA-1: Initial Quality Analysis - MANDATORY Playwright Steps**

```markdown
## QA-1: Playwright Visual Analysis (MANDATORY)

### Step 1: Navigate to Application
**Required Actions:**
- [ ] Use mcp__playwright__browser_navigate to open application URL
- [ ] Take initial screenshot with mcp__playwright__browser_take_screenshot
- [ ] Use mcp__playwright__browser_snapshot to get accessibility tree
- [ ] Document: URL accessed, initial load time, any console errors

### Step 2: Responsive Breakpoint Testing
**Required Actions:**
- [ ] Test Mobile (375x667): mcp__playwright__browser_resize width=375 height=667
- [ ] Take screenshot: filename="mobile_view.png"
- [ ] Test Tablet (768x1024): mcp__playwright__browser_resize width=768 height=1024  
- [ ] Take screenshot: filename="tablet_view.png"
- [ ] Test Desktop (1440x900): mcp__playwright__browser_resize width=1440 height=900
- [ ] Take screenshot: filename="desktop_view.png"
- [ ] Document: Layout breaks, content overflow, usability issues at each breakpoint

### Step 3: Interactive Element Testing
**Required Actions:**
- [ ] Identify all clickable elements with mcp__playwright__browser_snapshot
- [ ] Test primary navigation: mcp__playwright__browser_click on each menu item
- [ ] Test forms: mcp__playwright__browser_type in form fields, test validation
- [ ] Test buttons: hover states, click feedback, loading states
- [ ] Document: All interaction results, broken functionality, UX issues

### Step 4: Accessibility Validation
**Required Actions:**
- [ ] Use mcp__playwright__browser_snapshot to analyze accessibility tree
- [ ] Test keyboard navigation: mcp__playwright__browser_press_key with Tab, Enter, Space
- [ ] Check color contrast ratios using browser developer tools
- [ ] Verify ARIA labels and semantic HTML structure
- [ ] Document: Accessibility violations, keyboard navigation issues, contrast problems

### Evidence Documentation REQUIRED:
**For EACH dimension analysis, you MUST provide:**
1. **Screenshots** - Actual images showing current state
2. **Console Messages** - Any errors or warnings found
3. **Interaction Results** - What happened when testing features
4. **Specific Issues** - Exact problems identified with locations
```

##### **QA-3: 6-Dimension Scoring - MANDATORY Evidence-Based**

```markdown
## QA-3: Systematic Quality Scoring with Playwright Evidence

### Visual Consistency (20% weight) - Score: __/10
**Playwright Analysis Required:**
- [ ] Take screenshots of all major pages/sections
- [ ] Compare design system consistency across pages
- [ ] Check component reuse and styling uniformity
- [ ] Verify typography consistency (font sizes, weights, spacing)
- [ ] Document color usage consistency throughout

**Evidence:** [Attach screenshots showing design consistency/inconsistency]
**Issues Found:** [List specific visual inconsistencies with locations]
**Score Justification:** [Explain why this score based on evidence]

### CRO Optimization (25% weight) - Score: __/10  
**Playwright Analysis Required:**
- [ ] Navigate through user journey: Homepage ‚Üí Product ‚Üí Checkout
- [ ] Test call-to-action buttons: visibility, placement, contrast
- [ ] Analyze trust signals: testimonials, security badges, guarantees
- [ ] Test form usability: field labels, error messages, completion flow
- [ ] Check loading speed and user feedback during interactions

**Evidence:** [Screenshots of CTA buttons, user flow, trust signals]
**Conversion Barriers:** [List specific issues blocking conversions]
**Score Justification:** [Explain score based on CRO best practices found/missing]

### Accessibility (20% weight) - Score: __/10
**Playwright Analysis Required:**
- [ ] Full accessibility tree analysis with browser_snapshot
- [ ] Keyboard navigation testing: Tab order, focus indicators
- [ ] Color contrast measurement: Text/background ratios
- [ ] Screen reader compatibility: ARIA labels, semantic HTML
- [ ] Form accessibility: Labels, error messages, validation

**Evidence:** [Accessibility tree snapshots, keyboard navigation screenshots]
**WCAG Violations:** [List specific accessibility issues found]
**Score Justification:** [Based on WCAG 2.1 compliance level achieved]

### Architecture Quality (15% weight) - Score: __/10
**Playwright Analysis Required:**
- [ ] Analyze HTML structure quality from page snapshots
- [ ] Check CSS organization: class naming, structure
- [ ] Test page performance: loading times, resource optimization
- [ ] Verify responsive implementation quality
- [ ] Check JavaScript functionality and error handling

**Evidence:** [Code structure analysis, performance metrics]
**Technical Issues:** [Specific architectural problems found]
**Score Justification:** [Based on code quality and structure analysis]

### Performance (10% weight) - Score: __/10
**Playwright Analysis Required:**
- [ ] Measure page load times for different pages
- [ ] Check Core Web Vitals: LCP, FID, CLS
- [ ] Test image loading and optimization
- [ ] Analyze network requests and resource loading
- [ ] Test performance on different device sizes

**Evidence:** [Performance metrics, load time measurements]
**Performance Issues:** [Specific slow loading elements or processes]
**Score Justification:** [Based on actual performance measurements]

### Responsive Design (10% weight) - Score: __/10
**Playwright Analysis Required:**
- [ ] Test ALL breakpoints: 320px, 375px, 768px, 1024px, 1440px+
- [ ] Check layout adaptation at each breakpoint
- [ ] Verify touch targets on mobile (minimum 44px)
- [ ] Test horizontal scrolling issues
- [ ] Check content readability at all sizes

**Evidence:** [Screenshots at each breakpoint showing layout]
**Responsive Issues:** [Specific layout problems at different sizes]
**Score Justification:** [Based on responsive behavior quality]
```

##### **Mandatory Evidence Requirements**

#### **For EVERY Quality Dimension - Must Provide:**
1. **Playwright Command Used** - Exact MCP command executed
2. **Screenshot Evidence** - Visual proof of current state  
3. **Specific Issues List** - Exact problems found with locations
4. **Improvement Recommendations** - Specific fixes needed
5. **Score Justification** - Why this score based on evidence

#### **Playwright Commands Reference - MUST USE**
```bash
# Navigation and Setup
mcp__playwright__browser_navigate --url "http://localhost:3000"
mcp__playwright__browser_resize --width 375 --height 667

# Visual Analysis  
mcp__playwright__browser_take_screenshot --filename "analysis_desktop.png"
mcp__playwright__browser_snapshot  # Gets accessibility tree

# Interaction Testing
mcp__playwright__browser_click --element "navigation menu" --ref "[data-element-ref]"
mcp__playwright__browser_type --element "search input" --text "test query"
mcp__playwright__browser_press_key --key "Tab"

# Multiple Device Testing
mcp__playwright__browser_resize --width 320 --height 568  # Mobile small
mcp__playwright__browser_resize --width 768 --height 1024 # Tablet  
mcp__playwright__browser_resize --width 1440 --height 900 # Desktop
```

### **Quality Validation Enforcement**

#### **NO Quality Certification Without:**
1. **Actual Playwright usage** - Commands executed and documented
2. **Visual evidence** - Screenshots for each dimension analysis
3. **Specific issue identification** - Exact problems with locations
4. **Evidence-based scoring** - Scores justified by actual findings
5. **Improvement documentation** - Specific fixes for issues found

#### **Red Flags - Invalid Quality Analysis:**
- ‚ùå Quality scores without screenshots
- ‚ùå General statements without specific evidence
- ‚ùå No Playwright commands documented
- ‚ùå Subjective assessments without measurable criteria
- ‚ùå Missing responsive breakpoint testing

---

## üé® UI/UX Task Specification System (ISSUE-009 Resolution)

### CRITICAL: UI/UX Tasks Must Be Detailed and Specific

### **Problem with Vague UI/UX Tasks:**
‚ùå "Design homepage"  
‚ùå "Create user interface"  
‚ùå "Style the application"  

### **Required: Detailed UI/UX Task Breakdown**

#### **UI/UX Task Template (MANDATORY)**
```markdown
## UI Task: [Specific Component/Page/Feature]

### Design Deliverables Required
- [ ] **Wireframes** - Low-fidelity layout and structure
- [ ] **Visual Design** - High-fidelity mockups with colors, typography, spacing
- [ ] **Component Specifications** - Detailed specs for each UI element
- [ ] **Responsive Breakpoints** - Mobile, tablet, desktop variants
- [ ] **Interaction Design** - Hover states, animations, transitions
- [ ] **Accessibility Specs** - Color contrast, keyboard navigation, ARIA labels

### Component-Level Breakdown
**For each UI component, specify:**
1. **Purpose** - What this component does
2. **States** - Default, hover, active, disabled, loading
3. **Variants** - Size options, color schemes, layout variations
4. **Content** - Text, images, icons, data requirements
5. **Behavior** - Interactions, animations, responsive behavior
6. **Dependencies** - What other components it connects to

### Detailed Specifications Required

#### **Visual Design Specs:**
- **Typography** - Font families, sizes, weights, line heights
- **Color Palette** - Primary, secondary, accent, neutral colors
- **Spacing System** - Margins, paddings, gaps (8px grid recommended)
- **Border Radius** - Consistent rounding values
- **Shadows/Effects** - Drop shadows, gradients, overlays
- **Icons** - Style, size, usage guidelines

#### **Layout Specifications:**
- **Grid System** - Column layout, breakpoints, container widths
- **Component Hierarchy** - Header, main content, sidebar, footer
- **Navigation Structure** - Menu items, breadcrumbs, pagination
- **Content Sections** - Hero, features, testimonials, etc.
- **Spacing Rules** - Consistent vertical rhythm, component gaps

#### **Responsive Behavior:**
- **Mobile (320-768px)** - Single column, stacked elements, touch-friendly
- **Tablet (768-1024px)** - Two-column layout, medium spacing
- **Desktop (1024px+)** - Multi-column, optimal spacing, hover effects

#### **Interaction Design:**
- **Micro-interactions** - Button clicks, form submissions, loading states
- **Transitions** - Page changes, modal appearances, content reveals
- **Feedback** - Success messages, error states, progress indicators
- **Navigation** - Menu animations, scroll behaviors, breadcrumb updates
```

### **UI/UX Task Examples - Before vs After**

#### **‚ùå WRONG - Vague Task:**
```
Task 2.3: Design homepage
Acceptance Criteria: Homepage should look professional
Time Estimate: 8 hours
```

#### **‚úÖ CORRECT - Detailed Task:**
```
Task 2.3: Design Homepage Layout and Components
Time Estimate: 16 hours (broken into sub-tasks)

Sub-task 2.3.1: Homepage Wireframe Design (4 hours)
- [ ] Define page structure: Header, Hero, Features, Testimonials, Footer  
- [ ] Create low-fidelity wireframes for all breakpoints
- [ ] Specify content hierarchy and information architecture
- [ ] Define navigation flow and user journey
- [ ] Document component placement and relationships

Sub-task 2.3.2: Visual Design System (6 hours)
- [ ] Define color palette: Primary #2563EB, Secondary #64748B, Success #10B981
- [ ] Typography system: Headers (Inter Bold), Body (Inter Regular), sizes 14-48px
- [ ] Spacing system: 8px base grid (8, 16, 24, 32, 48, 64px)
- [ ] Component styles: Buttons, forms, cards, modals
- [ ] Icon set selection and customization

Sub-task 2.3.3: Homepage Component Design (6 hours)
- [ ] Hero Section: Background image, headline, CTA button, value proposition
- [ ] Navigation: Logo, menu items, mobile hamburger, search icon
- [ ] Features Grid: 3-column layout, icon+text cards, hover effects
- [ ] Testimonials Carousel: Customer photos, quotes, star ratings
- [ ] Footer: Links, social media, contact info, newsletter signup

Acceptance Criteria:
- [ ] All components have hover/active states defined
- [ ] Color contrast meets WCAG AA standards (4.5:1 minimum)
- [ ] Mobile-first responsive design implemented
- [ ] Typography hierarchy is clear and consistent
- [ ] Spacing follows 8px grid system throughout
- [ ] All interactive elements have defined states
```

### **Mandatory UI/UX Deliverables**

#### **For Every UI/UX Task - Must Include:**
1. **Design System Documentation**
   - Color palette with hex codes
   - Typography scale with font sizes
   - Spacing values and grid system
   - Component library specifications

2. **Component Specifications**
   - Visual appearance in all states
   - Responsive behavior at each breakpoint
   - Accessibility requirements (ARIA, contrast, keyboard)
   - Animation/interaction details

3. **User Experience Flow**
   - User journey mapping
   - Information architecture
   - Navigation patterns
   - Error and success states

4. **Implementation Guidelines**
   - CSS class naming conventions
   - HTML structure requirements
   - JavaScript interaction needs
   - Performance considerations

### **UI/UX Task Granularity Rules**

#### **Large UI Tasks ‚Üí Multiple Focused Tasks**
```
‚ùå "Design complete dashboard"
‚úÖ "Design dashboard navigation sidebar"
‚úÖ "Design dashboard main content area"
‚úÖ "Design dashboard statistics widgets"
‚úÖ "Design dashboard user profile section"
```

#### **Each Task Should Be 4-8 Hours Max**
- Focused on specific components or pages
- Clear, measurable deliverables
- Specific acceptance criteria
- Detailed visual and interaction specifications

### **Design Methodology Integration**

#### **Atomic Design Approach - Mandatory Structure:**
1. **Atoms** - Basic elements (buttons, inputs, labels)
2. **Molecules** - Component combinations (search bar, form groups)
3. **Organisms** - Complex components (header, product grid)
4. **Templates** - Page layouts and structures
5. **Pages** - Specific instances with real content

#### **Each Level Must Be Specified Separately:**
- Design atoms first with all states and variants
- Combine into molecules with interaction patterns
- Build organisms with responsive behavior
- Create templates with content guidelines
- Implement pages with real data and content

---

## ü§ñ AI Model Compatibility Guidelines

### Cross-Platform Tool Usage (ISSUE-006 Resolution)
Different AI models have different tool interfaces. The framework must accommodate these differences.

### **Claude Code Specific Tools**
- **Edit/Replace Commands:** Native support for find-and-replace operations
- **MultiEdit:** Batch editing capabilities
- **Tool Integration:** Full MCP integration available

### **Gemini CLI Compatibility Issues**
- **Replace Command FAILS:** Gemini does not support Claude Code's `replace` command
- **Alternative Required:** Must use different file editing approach
- **Tool Limitations:** Some Claude Code tools may not be available

### **Gemini CLI Workarounds**

#### **For File Editing (Replace replace command):**
```markdown
**INSTEAD OF:** Using replace command
**USE:** Manual file recreation approach

1. Read entire file content
2. Create new version with modifications  
3. Write complete new file content
4. Verify changes applied correctly

**Example Process:**
- Read current file: "Show me the current content of file.js"
- Manual editing: "Create new version with X changed to Y"
- Write new file: "Create file.js with this new content: [full content]"
```

#### **For Multi-File Operations:**
```markdown
**INSTEAD OF:** Batch operations
**USE:** Sequential individual file operations

1. Process one file at a time
2. Verify each change before proceeding
3. Commit changes incrementally
4. Maintain detailed log of modifications
```

### **AI Model Detection Protocol**
```markdown
**At Session Start - ALWAYS identify AI model being used:**

**For Claude:** 
- "I'm using Claude Code with full tool support"
- Use standard Edit/MultiEdit/Replace commands
- Full MCP integration available

**For Gemini:**
- "I'm using Gemini CLI with limited tool support"  
- Use manual file recreation for editing
- Alternative approaches for file operations
- Extra validation steps required

**For Other Models:**
- Identify tool capabilities early
- Test critical operations before proceeding
- Document working alternatives
- Add new compatibility guidelines
```

### **Mandatory Tool Compatibility Checks**
```markdown
**Before Starting Any File Operations:**

1. **Test Edit Capability**
   - Claude: Use Edit tool on test file
   - Gemini: Use manual file recreation test
   - Others: Identify working method

2. **Verify MCP Access**
   - Test Context7 research capability
   - Test Playwright integration
   - Document any limitations

3. **Establish Workflow**
   - Define file editing approach
   - Set validation procedures
   - Create fallback methods
```

### **Error Prevention for Tool Incompatibility**
- **Always identify AI model** at session start
- **Test critical tools** before beginning project work
- **Use model-appropriate** editing methods
- **Document workarounds** for future reference
- **Validate all changes** regardless of editing method used

### **Quality Assurance Regardless of AI Model**
- **Same DoD validation** applies to all models
- **Same tracking requirements** for all assistants
- **Same quality thresholds** (8/10+) for all projects
- **Tool differences don't affect** final quality standards

---

## üß† Error Memory System

### Framework Learning & Continuous Improvement
The framework includes a sophisticated error memory system to ensure continuous improvement and prevent recurring issues.

### **Core Memory Files**
- **KNOWN_ISSUES.md** - Complete database of discovered bugs, solutions, and prevention measures
- **FRAMEWORK.md** - This file, updated with lessons learned
- **Session logs** - Individual project experiences and outcomes

### **Error Memory Protocol for AI Assistants**

#### **1. Always Start with Memory Check**
```bash
# Before any framework operation:
1. Read KNOWN_ISSUES.md completely
2. Identify any relevant known issues
3. Apply documented solutions proactively
4. Watch for symptoms of documented problems
```

#### **2. Proactive Error Prevention**
- **Validate assumptions** - Never assume features work as documented
- **Test critical paths** - Verify Git setup, MCP tools, file permissions
- **Cross-reference docs** - Ensure README promises match implementation
- **Apply known workarounds** - Use documented solutions immediately

#### **3. Real-Time Issue Documentation**
When encountering ANY new problem:
```markdown
### ISSUE-XXX: [Brief Description]
**Status:** üîÑ INVESTIGATING
**Discovered:** [Date] during [Specific context]
**Problem:** [Detailed description]
**Symptoms:** [Observable behaviors]
**Current Workaround:** [Temporary solution]
**Next Steps:** [Investigation plan]
```

#### **4. Knowledge Sharing Requirements**
- **Document immediately** - Don't wait until "later"
- **Include context** - What were you doing when it happened?
- **Share solutions** - What worked to resolve it?
- **Update prevention** - How can we avoid this in future?

### **Error Categories & Response Protocols**

#### **Critical Errors (Framework Blocking)**
- **Git configuration failures** ‚Üí Apply ISSUE-001/002 solutions
- **MCP integration problems** ‚Üí Check tool installation and permissions
- **File permission issues** ‚Üí Validate ultra-flat structure requirements

#### **Workflow Inconsistencies** 
- **Documentation vs implementation gaps** ‚Üí Update both immediately
- **Missing features** ‚Üí Document as known limitation
- **Assistant behavior gaps** ‚Üí Add specific rules to Critical Rules

#### **Quality System Issues**
- **Healing loop failures** ‚Üí Check MCP Playwright setup
- **Context7 research problems** ‚Üí Implement graceful fallbacks
- **Scoring inconsistencies** ‚Üí Validate against ui_healing_standards.json

### **Success Metrics for Error Memory**
- **Issue Discovery Rate** - How quickly we find problems
- **Resolution Time** - How fast we implement solutions
- **Prevention Effectiveness** - How well we avoid recurring issues
- **Framework Reliability** - Overall system stability improvement

### **Learning Integration Requirements**
Every discovered issue MUST result in:
1. **Immediate documentation** in KNOWN_ISSUES.md
2. **Framework rule updates** if behavioral changes needed
3. **Prevention measures** added to relevant guides
4. **Cross-validation** of similar potential issues

---

## üìà Success Metrics

### Framework Efficiency
- **PRD Creation:** 15 minutes (was 45+ minutes)
- **Technical Decisions:** 5 minutes (was 30+ minutes)
- **Quality Achievement:** Automated (was manual)
- **Market Accuracy:** Real-time (was outdated)

### Quality Assurance
- **100% compliance** with 8/10+ standard
- **Professional certification** for all projects
- **Reduced technical debt** through iterative improvement
- **Enhanced user experience** through systematic optimization

---

## üîÆ Framework Roadmap

### Current Version (v3.1.1 Enhanced)
- ‚úÖ Smart contextual questioning
- ‚úÖ Context7 auto-completion
- ‚úÖ Realistic market estimates
- ‚úÖ Iterative quality loop
- ‚úÖ MCP Playwright integration

### Future Enhancements
- üîÑ Multi-language project support
- üîÑ Advanced analytics integration
- üîÑ Team collaboration features
- üîÑ Industry-specific templates

---

## üí° Tips for Maximum Efficiency

1. **Trust the automation** - the framework has been optimized for efficiency
2. **Use Context7 research** - it provides current, relevant information
3. **Let the quality loop run** - it ensures professional standards
4. **Follow the session workflow** - it maintains perfect traceability
5. **Document customizations** - for future reference and improvements

---

## üîÑ Framework Rules & Standards

### Critical Rules (ALWAYS follow)
1. **ALL documentation in English** - Never use other languages in files
2. **Session-based workflow** - Always maintain session traceability
3. **Quality threshold enforcement** - Never bypass 8/10 requirement
4. **Context7 integration** - Use real-time research for all decisions
5. **MCP Playwright validation** - Visual analysis is mandatory
6. **Explain before implementing** - Always provide short, concise explanation before proposing code changes
7. **Ultra-flat structure** - Maintain simplified repository architecture
8. **No complex automation** - Focus on core framework functionality
9. **Proactive commit reminders** - ALWAYS remind user to commit after completing any file generation or major milestone
10. **Git workflow integration** - Follow conventional commits with session-id traceability
11. **Error memory system** - Always check KNOWN_ISSUES.md first and document new issues immediately
12. **Execution tracking mandatory** - Create and maintain 03_report_{session-id}_{project-name}.md with real-time task completion updates
13. **DoD validation required** - NEVER mark any task complete without executing full Definition of Done validation protocol
14. **Try before escalating** - NEVER create impediment report without attempting at least 2 different solution approaches first
15. **Detailed UI/UX specification** - NEVER create vague UI/UX tasks, always break down into specific components with detailed deliverables
16. **Mandatory Playwright execution** - NEVER provide quality scores without actually executing Playwright commands and providing screenshot evidence

### Output Standards
- **File naming:** Follow exact pattern `{step}_{session-id}_{project-name}.md`
- **Language:** English only in all documentation and code
- **Quality:** Professional standards with certification
- **Traceability:** Perfect session linkage across all files

---

**Remember:** This framework is designed to deliver professional-quality results efficiently. Trust the process, use the automation, and focus on the creative and strategic aspects of development while the framework handles the systematic quality assurance.